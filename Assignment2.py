# -*- coding: utf-8 -*-
"""ASsignment 2 RDD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXFsEjfx1_9PS4ccvBOqCFIjLBME5WrD
"""

!pip install PySpark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.sql.functions import col, lower, regexp_replace,concat_ws, when, lit
from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer
from pyspark.ml import Pipeline
from pyspark.ml import classification
from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier, MultilayerPerceptronClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statistics as sts
import math
import re
import operator

spark = SparkSession.builder.appName('Assignment2').getOrCreate()

# from google.colab import drive
# drive.mount('/content/drive')

data = spark.read.csv('fake_job_postings.csv', header=True, inferSchema=True)

display(data.show())

# Step 3: Clean the dataset
# Encode the label column and remove invalid data
data = data.filter((col("fraudulent") == 0) | (col("fraudulent") == 1))
data = data.withColumn("fraudulent", col("fraudulent").cast("int"))

fraud_data = data[data['fraudulent'] == 1]  #fraudulent
not_fraud_data = data[data['fraudulent'] == 0]  # non fraudulent

def give_null_stats(data,fraud_data):

    column_names = data.columns

    # Create an empty list to store the results
    results = []
    original_null_count=0
    original_not_null_count=0
    # Loop through each column
    for column_name in column_names:
        # Calculate the count of null and non-null values in the column
        original_null_count = data.where(col(column_name).isNull()).count()
        original_not_null_count = data.where(col(column_name).isNotNull()).count()

        fraud_null_count = fraud_data.where(col(column_name).isNull()).count()
        fraud_not_null_count = fraud_data.where(col(column_name).isNotNull()).count()
        fraud_count=fraud_data.count()

        # Calculate the null percentage
        original_null_percentage = (original_null_count / data.count()) * 100
        fraud_null_precentage=(fraud_null_count/data.count()*100) # fraud null/ total data that is fraud null+ not null +
        # Append the results as a tuple (column name, null percentage)
        results.append((column_name, original_null_percentage,fraud_null_precentage))

    schema = StructType([
    StructField("Column_Name", StringType(), True),  # String column
    StructField("Original_Null", DoubleType(), True),  # Double column
    StructField("Fraud_Null", DoubleType(), True)  # Double column
    ])
    # Create a new DataFrame from the results
    data_null_stats = spark.createDataFrame(results, schema=schema)

    return data_null_stats

# Show the results DataFrame
data_stats=give_null_stats(data,fraud_data)
data_stats.show()
from pyspark.sql.functions import col

filtered_df = data_stats.where(col("Original_Null") > 1)
display(filtered_df.show())
column_name = "Column_Name"

# Select the column and collect its values as a list
column_values_list = filtered_df.select(column_name).rdd.flatMap(lambda x: x).collect()

# Show the list
print(len(column_values_list))

(column_values_list)
columns_to_keep = [col(column) for column in data.columns if column not in column_values_list]

# Select the columns to keep and create a new DataFrame
new_df = data.select(*columns_to_keep)
new_df=new_df.drop('job_id')
# Show the new DataFrame
new_df.show()

def clean_text(text):
    # Remove non-letter characters and reduce multiple spaces to a single space
    cleaned_text = regexp_replace(text, "[^a-zA-Z ]+", " ").alias("cleaned_text")
    # Convert to lowercase
    cleaned_text = lower(cleaned_text)
    return cleaned_text

# Apply the cleaning function to 'title' and 'description' column
def replace_non_binary_with_zero(df, column_name):
    """
    Check if a column in a PySpark DataFrame has data other than '0' or '1' and replace
    non-binary values with '0'.

    Args:
        df (DataFrame): The PySpark DataFrame.
        column_name (str): The name of the column to check and modify.

    Returns:
        DataFrame: The DataFrame with non-binary values replaced by '0'.
    """
    # Define a condition to check for non-binary values
    condition = (~(col(column_name) == '0') & ~(col(column_name) == '1'))

    # Replace non-binary values with '0'
    df = df.withColumn(column_name, when(condition, lit('0')).otherwise(col(column_name)))

    return df

cleaned_df = new_df.withColumn("title", clean_text(col("title"))) \
               .withColumn("description", clean_text(col("description")))

cleaned_df= replace_non_binary_with_zero(cleaned_df,'telecommuting')
cleaned_df= replace_non_binary_with_zero(cleaned_df,'has_company_logo')
cleaned_df= replace_non_binary_with_zero(cleaned_df,'has_questions')

# Show the cleaned DataFrame
display(cleaned_df.show())

column_names_to_check = ["telecommuting", "has_company_logo", "has_questions"]

# Initialize a flag to track non-binary values
has_non_binary_values = False

# Iterate through the column names and check for non-binary values
for column_name in column_names_to_check:
    non_binary_values_count = cleaned_df.filter(~col(column_name).isin(['0', '1'])).count()
    if non_binary_values_count > 0:
        print(f"The column '{column_name}' contains values other than '0' or '1'.")
        has_non_binary_values = True

# If no column contains non-binary values, print a message
if not has_non_binary_values:
    print("All columns only contain '0' and '1'.")

undersampling_ratio = 0.0565

# Split the DataFrame into two DataFrames: one for each class
fraudulent_df = cleaned_df.filter(col("fraudulent") == 1)
non_fraudulent_df = cleaned_df.filter(col("fraudulent") == 0)

# Undersample the majority class (non-fraudulent)
undersampled_non_fraudulent_df = non_fraudulent_df.sampleBy("fraudulent", fractions={0: undersampling_ratio, 1: 1.0})

# Combine the undersampled majority class with the minority class
undersampled_df = fraudulent_df.union(undersampled_non_fraudulent_df)

# Show the resulting undersampled DataFrame
undersampled_df.show()

print(undersampled_df.where(col("fraudulent") == 1).count())
print(undersampled_df.where(col("fraudulent") == 0).count())

data_rdd=undersampled_df.rdd
data_rdd.toDF().printSchema()

data_rdd_df = data_rdd.toDF()

# Concatenate "title" and "description" into a single text column
data_rdd_df = data_rdd_df.withColumn("title_description", concat_ws(" ", col("title"), col("description")))

# Tokenize text
tokenizer = Tokenizer(inputCol="title_description", outputCol="words")
tokenize_df = tokenizer.transform(data_rdd_df)

# Remove stopwords
stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
tokenize_stopwords_df = stopwords_remover.transform(tokenize_df)

# Convert text to vectors using CountVectorizer
vectorizer = CountVectorizer(inputCol="filtered_words", outputCol="features")
model = vectorizer.fit(tokenize_stopwords_df)
tokenize_stopwords_vectorized_df = model.transform(tokenize_stopwords_df)

(training_data, test_data) = tokenize_stopwords_vectorized_df.randomSplit([0.7, 0.3], seed=42)

"""Default Hyperparameters:"""

#taking hidden layers for mlp
input_size = len(training_data.select("features").first()[0])
layers = [input_size, 64, 32, 2]

#training all the models
lr_model_base = classification.LogisticRegression(featuresCol="features", labelCol="fraudulent")
lsvc_model_base = classification.LinearSVC(featuresCol="features", labelCol="fraudulent")
rf_model_base = classification.RandomForestClassifier(featuresCol="features", labelCol="fraudulent")
mlp_model_base = classification.MultilayerPerceptronClassifier(featuresCol="features", labelCol="fraudulent", layers=layers)

#fitting all the models
lr_model_base_fit = lr_model_base.fit(training_data)
lsvc_model_base_fit = lsvc_model_base.fit(training_data)
rf_model_base_fit = rf_model_base.fit(training_data)
mlp_model_base_fit = mlp_model_base.fit(training_data)

evaluator_base = BinaryClassificationEvaluator(labelCol="fraudulent")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="fraudulent", predictionCol="prediction", metricName="f1")

#evaluating accuracy
lr_model_base_fit_auc = evaluator_base.evaluate(lr_model_base_fit.transform(test_data))
lsvc_model_base_fit_auc = evaluator_base.evaluate(lsvc_model_base_fit.transform(test_data))
rf_model_base_fit_auc = evaluator_base.evaluate(rf_model_base_fit.transform(test_data))
mlp_model_base_fit_auc = evaluator_base.evaluate(mlp_model_base_fit.transform(test_data))

#getting out the predictions of all the models
lr_base_predictions = lr_model_base_fit.transform(test_data)
lsvc_base_predictions = lsvc_model_base_fit.transform(test_data)
rf_base_predictions = rf_model_base_fit.transform(test_data)
mlp_base_predictions = mlp_model_base_fit.transform(test_data)

#evaluating f1 score
lr_base_f1_score = evaluator_f1.evaluate(lr_base_predictions)
lsvc_base_f1_score = evaluator_f1.evaluate(lsvc_base_predictions)
rf_base_f1_score = evaluator_f1.evaluate(rf_base_predictions)
mlp_base_f1_score = evaluator_f1.evaluate(mlp_base_predictions)

#printing all AUC scores & f1-scores for each model
print(f"Logistic Regression AUC:{lr_model_base_fit_auc}")
print(f"Logistic Regression F1-Score: {lr_base_f1_score} \n\n")

print(f"Linear SVC AUC: {lsvc_model_base_fit_auc}")
print(f"Linear SVC F1-Score: {lsvc_base_f1_score} \n\n")

print(f"Random Forest AUC: {rf_model_base_fit_auc} ")
print(f"Random Forest F1-Score: {rf_base_f1_score} \n\n")

print(f"Multilayer Perceptron AUC: {mlp_model_base_fit_auc}")
print(f"Multilayer Perceptron F1-Score: {mlp_base_f1_score} \n\n")

"""With Cross Validation and tuned Hyperparameters:"""

evaluator = BinaryClassificationEvaluator(labelCol="fraudulent")

lr = LogisticRegression(featuresCol="features", labelCol="fraudulent")
lsvc = LinearSVC(featuresCol="features", labelCol="fraudulent")
rf = RandomForestClassifier(featuresCol="features", labelCol="fraudulent")
mlp = MultilayerPerceptronClassifier(featuresCol="features", labelCol="fraudulent")

#paramGrid = ParamGridBuilder().build()

lr_param_grid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .addGrid(lr.maxIter, [10, 20, 30]) \
    .build()

lsvc_param_grid = ParamGridBuilder() \
    .addGrid(lsvc.maxIter, [10, 20, 30]) \
    .addGrid(lsvc.regParam, [0.01, 0.1, 1.0]) \
    .build()

rf_param_grid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [50, 100, 200]) \
    .addGrid(rf.maxDepth, [5, 10, 15]) \
    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \
    .build()

mlp_param_grid = ParamGridBuilder() \
    .addGrid(mlp.maxIter, [10, 20, 30]) \
    .addGrid(mlp.blockSize, [128, 256, 512]) \
    .build()

# Cross-validation
crossval_lr = CrossValidator(estimator=lr, estimatorParamMaps=lr_param_grid, evaluator=evaluator, numFolds=10)
crossval_lsvc = CrossValidator(estimator=lsvc, estimatorParamMaps=lsvc_param_grid, evaluator=evaluator, numFolds=10)
crossval_rf = CrossValidator(estimator=rf, estimatorParamMaps=rf_param_grid, evaluator=evaluator, numFolds=10)
crossval_mlp = CrossValidator(estimator=mlp, estimatorParamMaps=mlp_param_grid, evaluator=evaluator, numFolds=10)

lr = LogisticRegression(featuresCol="features", labelCol="fraudulent")
lr_model = crossval_lr.fit(training_data)
lr_predictions = lr_model.transform(test_data)
lr_auc = evaluator.evaluate(lr_predictions)
lr_f1 = evaluator_f1.evaluate(lr_predictions)
lr_best_params = lr_model.bestModel.extractParamMap()
print(f"Logistic Regression AUC: {lr_auc}")
print(f"Logistic Regression F1-SCORE: {lr_f1}")
print(f"Best Params for Logistic Regression: {lr_best_params}")

lsvc = LinearSVC(featuresCol="features", labelCol="fraudulent")
lsvc_model = crossval_lsvc.fit(training_data)
lsvc_predictions = lsvc_model.transform(test_data)
lsvc_auc = evaluator.evaluate(lsvc_predictions)
lsvc_f1 = evaluator_f1.evaluate(lsvc_predictions)
lsvc_best_params = lsvc_model.bestModel.extractParamMap()
print(f"Linear SVC AUC: {lsvc_auc}")
print(f"Linear SVC F1-Score: {lsvc_f1}")
print(f"Best Params for Linear SVC: {lsvc_best_params}")

rf = RandomForestClassifier(featuresCol="features", labelCol="fraudulent")
rf_model = crossval_rf.fit(training_data)
rf_predictions = rf_model.transform(test_data)
rf_auc = evaluator.evaluate(rf_predictions)
rf_f1 = evaluator_f1.evaluate(rf_predictions)
rf_best_params = rf_model.bestModel.extractParamMap()
print(f"Random Forest AUC: {rf_auc}")
print(f"Random Forest F1-Score: {rf_f1}")
print(f"Best Params for Random Forest: {rf_best_params}")

input_size = len(training_data.select("features").first()[0])
layers = [input_size, 64, 32, 2]  # Adjust the size of hidden layers as needed

# Create the MultilayerPerceptronClassifier with the "layers" parameter
mlp = MultilayerPerceptronClassifier(featuresCol="features", labelCol="fraudulent", layers=layers)
crossval_mlp = CrossValidator(estimator=mlp, estimatorParamMaps=mlp_param_grid, evaluator=evaluator, numFolds=10)
# Rest of the code for cross-validation, training, and evaluation remains the same
mlp_model = crossval_mlp.fit(training_data)
mlp_predictions = mlp_model.transform(test_data)
mlp_auc = evaluator.evaluate(mlp_predictions)
mlp_f1 = evaluator_f1.evaluate(mlp_predictions)
mlp_best_params = mlp_model.bestModel.extractParamMap()
print(f"Multilayer Perceptron AUC: {mlp_auc}")
print(f"Multilayer Perceptron F1-Score: {mlp_f1}")
print(f"Best Params for Multilayer Perceptron: {mlp_best_params}")

