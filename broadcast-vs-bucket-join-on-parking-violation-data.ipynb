{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-22T00:20:45.295894Z","iopub.execute_input":"2023-09-22T00:20:45.296775Z","iopub.status.idle":"2023-09-22T00:20:45.657890Z","shell.execute_reply.started":"2023-09-22T00:20:45.296738Z","shell.execute_reply":"2023-09-22T00:20:45.655943Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\n/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\n/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2014__August_2013___June_2014_.csv\n/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:20:45.659803Z","iopub.execute_input":"2023-09-22T00:20:45.660278Z","iopub.status.idle":"2023-09-22T00:21:36.059092Z","shell.execute_reply.started":"2023-09-22T00:20:45.660240Z","shell.execute_reply":"2023-09-22T00:21:36.057958Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=c8e1266e4ca1fd2ca0ae252bf943d42f4762fea25a829a2f559ed17609fab767\n  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import *\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:21:36.063264Z","iopub.execute_input":"2023-09-22T00:21:36.063639Z","iopub.status.idle":"2023-09-22T00:21:36.157544Z","shell.execute_reply.started":"2023-09-22T00:21:36.063599Z","shell.execute_reply":"2023-09-22T00:21:36.156607Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T03:11:13.476834Z","iopub.execute_input":"2023-09-22T03:11:13.477351Z","iopub.status.idle":"2023-09-22T03:11:13.491886Z","shell.execute_reply.started":"2023-09-22T03:11:13.477309Z","shell.execute_reply":"2023-09-22T03:11:13.490631Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:32:36.754706Z","iopub.execute_input":"2023-09-22T00:32:36.755141Z","iopub.status.idle":"2023-09-22T00:32:36.765154Z","shell.execute_reply.started":"2023-09-22T00:32:36.755108Z","shell.execute_reply":"2023-09-22T00:32:36.764099Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:41:40.179379Z","iopub.execute_input":"2023-09-22T00:41:40.180435Z","iopub.status.idle":"2023-09-22T00:41:41.152191Z","shell.execute_reply.started":"2023-09-22T00:41:40.180384Z","shell.execute_reply":"2023-09-22T00:41:41.150884Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Original Code","metadata":{}},{"cell_type":"markdown","source":"Bucket Size 400","metadata":{}},{"cell_type":"code","source":"start_time1 = time.time()\nparkViolations_2015 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\")\nparkViolations_2016 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\")\nnew_column_name_list= list(map(lambda x: x.replace(\" \", \"_\"), parkViolations_2015.columns))\nparkViolations_2015 = parkViolations_2015.toDF(*new_column_name_list)\nparkViolations_2015 = parkViolations_2015.filter(parkViolations_2015.Plate_Type == \"COM\").filter(parkViolations_2015.Vehicle_Year == \"2001\")\nparkViolations_2016 = parkViolations_2016.toDF(*new_column_name_list)\nparkViolations_2016 = parkViolations_2016.filter(parkViolations_2016.Plate_Type == \"COM\").filter(parkViolations_2016.Vehicle_Year == \"2001\")\nparkViolations_2015.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2015\")\nparkViolations_2016.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2016\")\nparkViolations_2015_tbl = spark.read.table(\"parkViolations_bkt_2015\")\nparkViolations_2016_tbl = spark.read.table(\"parkViolations_bkt_2016\")\nstart_time2 = time.time()\njoinDF = parkViolations_2015_tbl.join(parkViolations_2016_tbl, (parkViolations_2015_tbl.Plate_Type ==  parkViolations_2016_tbl.Plate_Type) & (parkViolations_2015_tbl.Vehicle_Year ==  parkViolations_2016_tbl.Vehicle_Year) , \"inner\").select(parkViolations_2015_tbl[\"Summons_Number\"], parkViolations_2016_tbl[\"Issue_Date\"])\ntime_taken_by_join = time.time() - start_time2\njoinDF.explain()\njoinDF.count()\n#joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/kaggle/working/bkt_joined_df.csv\")\ntime_taken_for_all = time.time() - start_time1\nprint(\"Time Taken only by Join:\", time_taken_by_join, \"seconds\")\nprint(\"Time Taken for the whole code to run:\", time_taken_for_all, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:41:55.687748Z","iopub.execute_input":"2023-09-22T00:41:55.688133Z","iopub.status.idle":"2023-09-22T00:43:34.166165Z","shell.execute_reply.started":"2023-09-22T00:41:55.688103Z","shell.execute_reply":"2023-09-22T00:43:34.165201Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [Summons_Number#2976, Issue_Date#3082]\n   +- SortMergeJoin [Vehicle_Year#3011, Plate_Type#2979], [Vehicle_Year#3113, Plate_Type#3081], Inner\n      :- Sort [Vehicle_Year#3011 ASC NULLS FIRST, Plate_Type#2979 ASC NULLS FIRST], false, 0\n      :  +- Filter (isnotnull(Plate_Type#2979) AND isnotnull(Vehicle_Year#3011))\n      :     +- FileScan parquet spark_catalog.default.parkviolations_bkt_2015[Summons_Number#2976,Plate_Type#2979,Vehicle_Year#3011] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#2979), isnotnull(Vehicle_Year#3011)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2015], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Summons_Number:string,Plate_Type:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n      +- Sort [Vehicle_Year#3113 ASC NULLS FIRST, Plate_Type#3081 ASC NULLS FIRST], false, 0\n         +- Filter (isnotnull(Plate_Type#3081) AND isnotnull(Vehicle_Year#3113))\n            +- FileScan parquet spark_catalog.default.parkviolations_bkt_2016[Plate_Type#3081,Issue_Date#3082,Vehicle_Year#3113] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#3081), isnotnull(Vehicle_Year#3113)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2016], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Plate_Type:string,Issue_Date:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 14:=====================================================>(399 + 1) / 400]\r","output_type":"stream"},{"name":"stdout","text":"Time Taken only by Join: 0.03740334510803223 seconds\nTime Taken for the whole code to run: 98.46608924865723 seconds\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"Bucket Size 200","metadata":{}},{"cell_type":"code","source":"start_time1 = time.time()\nparkViolations_2015 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\")\nparkViolations_2016 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\")\nnew_column_name_list= list(map(lambda x: x.replace(\" \", \"_\"), parkViolations_2015.columns))\nparkViolations_2015 = parkViolations_2015.toDF(*new_column_name_list)\nparkViolations_2015 = parkViolations_2015.filter(parkViolations_2015.Plate_Type == \"COM\").filter(parkViolations_2015.Vehicle_Year == \"2001\")\nparkViolations_2016 = parkViolations_2016.toDF(*new_column_name_list)\nparkViolations_2016 = parkViolations_2016.filter(parkViolations_2016.Plate_Type == \"COM\").filter(parkViolations_2016.Vehicle_Year == \"2001\")\nparkViolations_2015.write.mode(\"overwrite\").bucketBy(200, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2015\")\nparkViolations_2016.write.mode(\"overwrite\").bucketBy(200, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2016\")\nparkViolations_2015_tbl = spark.read.table(\"parkViolations_bkt_2015\")\nparkViolations_2016_tbl = spark.read.table(\"parkViolations_bkt_2016\")\nstart_time2 = time.time()\njoinDF = parkViolations_2015_tbl.join(parkViolations_2016_tbl, (parkViolations_2015_tbl.Plate_Type ==  parkViolations_2016_tbl.Plate_Type) & (parkViolations_2015_tbl.Vehicle_Year ==  parkViolations_2016_tbl.Vehicle_Year) , \"inner\").select(parkViolations_2015_tbl[\"Summons_Number\"], parkViolations_2016_tbl[\"Issue_Date\"])\ntime_taken_by_join = time.time() - start_time2\njoinDF.explain()\njoinDF.count()\n#joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/kaggle/working/bkt_joined_df.csv\")\ntime_taken_for_all = time.time() - start_time1\nprint(\"Time Taken only by Join:\", time_taken_by_join, \"seconds\")\nprint(\"Time Taken for the whole code to run:\", time_taken_for_all, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:44:09.245368Z","iopub.execute_input":"2023-09-22T00:44:09.246340Z","iopub.status.idle":"2023-09-22T00:45:37.449046Z","shell.execute_reply.started":"2023-09-22T00:44:09.246303Z","shell.execute_reply":"2023-09-22T00:45:37.448027Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [Summons_Number#4145, Issue_Date#4251]\n   +- SortMergeJoin [Vehicle_Year#4180, Plate_Type#4148], [Vehicle_Year#4282, Plate_Type#4250], Inner\n      :- Sort [Vehicle_Year#4180 ASC NULLS FIRST, Plate_Type#4148 ASC NULLS FIRST], false, 0\n      :  +- Filter (isnotnull(Plate_Type#4148) AND isnotnull(Vehicle_Year#4180))\n      :     +- FileScan parquet spark_catalog.default.parkviolations_bkt_2015[Summons_Number#4145,Plate_Type#4148,Vehicle_Year#4180] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#4148), isnotnull(Vehicle_Year#4180)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2015], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Summons_Number:string,Plate_Type:string,Vehicle_Year:string>, SelectedBucketsCount: 200 out of 200\n      +- Sort [Vehicle_Year#4282 ASC NULLS FIRST, Plate_Type#4250 ASC NULLS FIRST], false, 0\n         +- Filter (isnotnull(Plate_Type#4250) AND isnotnull(Vehicle_Year#4282))\n            +- FileScan parquet spark_catalog.default.parkviolations_bkt_2016[Plate_Type#4250,Issue_Date#4251,Vehicle_Year#4282] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#4250), isnotnull(Vehicle_Year#4282)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2016], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Plate_Type:string,Issue_Date:string,Vehicle_Year:string>, SelectedBucketsCount: 200 out of 200\n\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 21:=====================================================>(199 + 1) / 200]\r","output_type":"stream"},{"name":"stdout","text":"Time Taken only by Join: 0.03660011291503906 seconds\nTime Taken for the whole code to run: 88.18751358985901 seconds\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"Bucket Size 800","metadata":{}},{"cell_type":"code","source":"start_time1 = time.time()\nparkViolations_2015 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\")\nparkViolations_2016 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\")\nnew_column_name_list= list(map(lambda x: x.replace(\" \", \"_\"), parkViolations_2015.columns))\nparkViolations_2015 = parkViolations_2015.toDF(*new_column_name_list)\nparkViolations_2015 = parkViolations_2015.filter(parkViolations_2015.Plate_Type == \"COM\").filter(parkViolations_2015.Vehicle_Year == \"2001\")\nparkViolations_2016 = parkViolations_2016.toDF(*new_column_name_list)\nparkViolations_2016 = parkViolations_2016.filter(parkViolations_2016.Plate_Type == \"COM\").filter(parkViolations_2016.Vehicle_Year == \"2001\")\nparkViolations_2015.write.mode(\"overwrite\").bucketBy(800, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2015\")\nparkViolations_2016.write.mode(\"overwrite\").bucketBy(800, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2016\")\nparkViolations_2015_tbl = spark.read.table(\"parkViolations_bkt_2015\")\nparkViolations_2016_tbl = spark.read.table(\"parkViolations_bkt_2016\")\nstart_time2 = time.time()\njoinDF = parkViolations_2015_tbl.join(parkViolations_2016_tbl, (parkViolations_2015_tbl.Plate_Type ==  parkViolations_2016_tbl.Plate_Type) & (parkViolations_2015_tbl.Vehicle_Year ==  parkViolations_2016_tbl.Vehicle_Year) , \"inner\").select(parkViolations_2015_tbl[\"Summons_Number\"], parkViolations_2016_tbl[\"Issue_Date\"])\ntime_taken_by_join = time.time() - start_time2\njoinDF.explain()\njoinDF.count()\n#joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/kaggle/working/bkt_joined_df.csv\")\ntime_taken_for_all = time.time() - start_time1\nprint(\"Time Taken only by Join:\", time_taken_by_join, \"seconds\")\nprint(\"Time Taken for the whole code to run:\", time_taken_for_all, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T00:45:37.451325Z","iopub.execute_input":"2023-09-22T00:45:37.451707Z","iopub.status.idle":"2023-09-22T00:47:08.599546Z","shell.execute_reply.started":"2023-09-22T00:45:37.451671Z","shell.execute_reply":"2023-09-22T00:47:08.598535Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [Summons_Number#5314, Issue_Date#5420]\n   +- SortMergeJoin [Vehicle_Year#5349, Plate_Type#5317], [Vehicle_Year#5451, Plate_Type#5419], Inner\n      :- Sort [Vehicle_Year#5349 ASC NULLS FIRST, Plate_Type#5317 ASC NULLS FIRST], false, 0\n      :  +- Filter (isnotnull(Plate_Type#5317) AND isnotnull(Vehicle_Year#5349))\n      :     +- FileScan parquet spark_catalog.default.parkviolations_bkt_2015[Summons_Number#5314,Plate_Type#5317,Vehicle_Year#5349] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#5317), isnotnull(Vehicle_Year#5349)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2015], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Summons_Number:string,Plate_Type:string,Vehicle_Year:string>, SelectedBucketsCount: 800 out of 800\n      +- Sort [Vehicle_Year#5451 ASC NULLS FIRST, Plate_Type#5419 ASC NULLS FIRST], false, 0\n         +- Filter (isnotnull(Plate_Type#5419) AND isnotnull(Vehicle_Year#5451))\n            +- FileScan parquet spark_catalog.default.parkviolations_bkt_2016[Plate_Type#5419,Issue_Date#5420,Vehicle_Year#5451] Batched: true, Bucketed: true, DataFilters: [isnotnull(Plate_Type#5419), isnotnull(Vehicle_Year#5451)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/kaggle/working/spark-warehouse/parkviolations_bkt_2016], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Plate_Type:string,Issue_Date:string,Vehicle_Year:string>, SelectedBucketsCount: 800 out of 800\n\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 28:=====================================================>(799 + 1) / 800]\r","output_type":"stream"},{"name":"stdout","text":"Time Taken only by Join: 0.03946566581726074 seconds\nTime Taken for the whole code to run: 91.12543487548828 seconds\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T02:40:19.385882Z","iopub.execute_input":"2023-09-22T02:40:19.386414Z","iopub.status.idle":"2023-09-22T02:40:19.395755Z","shell.execute_reply.started":"2023-09-22T02:40:19.386370Z","shell.execute_reply":"2023-09-22T02:40:19.394544Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#BroadcastJoin\n\nstart_time1 = time.time()\nparkViolations_2015 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\")\nparkViolations_2016 = spark.read.option(\"header\", True).csv(\"/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\")\nparkViolations_2015 = parkViolations_2015.withColumnRenamed(\"Plate Type\", \"plateType\") # simple column rename for easier joins\nparkViolations_2016 = parkViolations_2016.withColumnRenamed(\"Plate Type\", \"plateType\")\n\nparkViolations_2015_COM = parkViolations_2015.filter(parkViolations_2015.plateType == \"COM\").select(\"plateType\", \"Summons Number\").distinct()\nparkViolations_2016_COM = parkViolations_2016.filter(parkViolations_2016.plateType == \"COM\").select(\"plateType\", \"Issue Date\").distinct()\nparkViolations_2015_COM.cache()\nparkViolations_2016_COM.cache()\nparkViolations_2015_COM.count() # will cause parkViolations_2015_COM to be cached\nparkViolations_2016_COM.count() # will cause parkViolations_2016_COM to be cached\nstart_time2 = time.time()\njoinDF = parkViolations_2015_COM.join(parkViolations_2016_COM.hint(\"broadcast\"), parkViolations_2015_COM.plateType ==  parkViolations_2016_COM.plateType, \"inner\").select(parkViolations_2015_COM[\"Summons Number\"], parkViolations_2016_COM[\"Issue Date\"])\ntime_taken_by_join = time.time() - start_time2\njoinDF.explain() # you will see BroadcastHashJoin\njoinDF.count()\n#joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/user/kfnu/output/joined_df\")\n#exit()\ntime_taken_for_all = time.time() - start_time1\nprint(\"Time Taken only by Join:\", time_taken_by_join, \"seconds\")\nprint(\"Time Taken for the whole code to run:\", time_taken_for_all, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T02:41:14.018553Z","iopub.execute_input":"2023-09-22T02:41:14.019074Z","iopub.status.idle":"2023-09-22T02:44:15.391808Z","shell.execute_reply.started":"2023-09-22T02:41:14.019030Z","shell.execute_reply":"2023-09-22T02:44:15.389545Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [Summons Number#9399, Issue Date#9522]\n   +- BroadcastHashJoin [plateType#9620], [plateType#9673], Inner, BuildRight, false\n      :- Filter isnotnull(plateType#9620)\n      :  +- InMemoryTableScan [plateType#9620, Summons Number#9399], [isnotnull(plateType#9620)]\n      :        +- InMemoryRelation [plateType#9620, Summons Number#9399], StorageLevel(disk, memory, deserialized, 1 replicas)\n      :              +- *(2) HashAggregate(keys=[plateType#9620, Summons Number#9399], functions=[])\n      :                 +- Exchange hashpartitioning(plateType#9620, Summons Number#9399, 200), ENSURE_REQUIREMENTS, [plan_id=2316]\n      :                    +- *(1) HashAggregate(keys=[plateType#9620, Summons Number#9399], functions=[])\n      :                       +- *(1) Project [Plate Type#9402 AS plateType#9620, Summons Number#9399]\n      :                          +- *(1) Filter (isnotnull(Plate Type#9402) AND (Plate Type#9402 = COM))\n      :                             +- FileScan csv [Summons Number#9399,Plate Type#9402] Batched: false, DataFilters: [isnotnull(Plate Type#9402), (Plate Type#9402 = COM)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fis..., PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Summons Number:string,Plate Type:string>\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2307]\n         +- Filter isnotnull(plateType#9673)\n            +- InMemoryTableScan [plateType#9673, Issue Date#9522], [isnotnull(plateType#9673)]\n                  +- InMemoryRelation [plateType#9673, Issue Date#9522], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- *(2) HashAggregate(keys=[plateType#9673, Issue Date#9522], functions=[])\n                           +- Exchange hashpartitioning(plateType#9673, Issue Date#9522, 200), ENSURE_REQUIREMENTS, [plan_id=2326]\n                              +- *(1) HashAggregate(keys=[plateType#9673, Issue Date#9522], functions=[])\n                                 +- *(1) Project [Plate Type#9521 AS plateType#9673, Issue Date#9522]\n                                    +- *(1) Filter (isnotnull(Plate Type#9521) AND (Plate Type#9521 = COM))\n                                       +- FileScan csv [Plate Type#9521,Issue Date#9522] Batched: false, DataFilters: [isnotnull(Plate Type#9521), (Plate Type#9521 = COM)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fis..., PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Plate Type:string,Issue Date:string>\n\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 44:>                 (0 + 1) / 1][Stage 76:=============>(198 + 1) / 200]\r","output_type":"stream"},{"name":"stdout","text":"Time Taken only by Join: 0.058622121810913086 seconds\nTime Taken for the whole code to run: 181.35094141960144 seconds\n","output_type":"stream"},{"name":"stderr","text":"[Stage 44:>                                                         (0 + 1) / 1]\r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}