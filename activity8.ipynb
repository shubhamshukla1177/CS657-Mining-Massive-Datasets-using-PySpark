{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(conf = SparkConf())\n",
    "spark = SparkSession.builder.appName(\"ass8\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import binascii\n",
    "from bisect import bisect_right\n",
    "from heapq import heappop, heappush\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def makeA(d):\n",
    "    dd = defaultdict(int, d)\n",
    "    return [dd[n] for n in range(len(d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"data/\"\n",
    "#df = spark.read \\\n",
    "#    .option(\"header\", False) \\\n",
    "#    .option(\"multiline\", True) \\\n",
    "#    .option(\"inferSchema\", True) \\\n",
    "#    .text('testdatahw1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numHashes = 10\n",
    "numDocs = 18506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling articles...\n",
      "\n",
      "Shingling 18506 docs took 3.06 sec.\n",
      "\n",
      "Average shingles per doc: 100.18\n"
     ]
    }
   ],
   "source": [
    "print(\"Shingling articles...\")\n",
    "\n",
    "# The current shingle ID value to assign to the next new shingle we \n",
    "# encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "# value.\n",
    "curShingleID = 0\n",
    "\n",
    "# Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "# \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "docsAsShingleSets = {};\n",
    "  \n",
    "# Open the data file.\n",
    "f = open('testdatahw1.txt', \"r\")\n",
    "\n",
    "docNames = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "totalShingles = 0\n",
    "\n",
    "for i in range(0, numDocs):\n",
    "  \n",
    "  # Read all of the words (they are all on one line) and split them by white\n",
    "  # space.\n",
    "  words = f.readline().split(\" \") \n",
    "  \n",
    "  # Retrieve the article ID, which is the first word on the line.  \n",
    "#  docID = words[0]\n",
    "  docID = i\n",
    "  \n",
    "  # Maintain a list of all document IDs.  \n",
    "  docNames.append(docID)\n",
    "    \n",
    "  del words[0]  \n",
    "  \n",
    "  # 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
    "  # current document. If a shingle ID occurs multiple times in the document,\n",
    "  # it will only appear once in the set (this is a property of Python sets).\n",
    "  shinglesInDoc = set()\n",
    "  \n",
    "  # For each word in the document...\n",
    "  for index in range(0, len(words) - 2):\n",
    "    # Construct the shingle text by combining three words together.\n",
    "    shingle = words[index] + \" \" + words[index + 1] + \" \" + words[index + 2]\n",
    "    \n",
    "    # Hash the shingle to a 32-bit integer.\n",
    "    crc = binascii.crc32(shingle.encode()) & 0xffffffff\n",
    "#    crc = binascii.crc32(shingle) & 0xffffffff\n",
    "#    crc = binascii.a2b_base64(shingle + \"=\")\n",
    "    \n",
    "    # Add the hash value to the list of shingles for the current document. \n",
    "    # Note that set objects will only add the value to the set if the set \n",
    "    # doesn't already contain it. \n",
    "    shinglesInDoc.add(crc)\n",
    "  \n",
    "  # Store the completed list of shingles for this document in the dictionary.\n",
    "  docsAsShingleSets[docID] = shinglesInDoc\n",
    "  \n",
    "  # Count the number of shingles across all documents.\n",
    "  totalShingles = totalShingles + (len(words) - 2)\n",
    "\n",
    "# Close the data file.  \n",
    "f.close()  \n",
    "\n",
    "# Report how long shingling took.\n",
    "print( '\\nShingling ' + str(numDocs) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    " \n",
    "print( '\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done = makeA(docsAsShingleSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = np.array(df_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = pd.DataFrame(df_final, columns = ['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(pd.arrays.SparseArray(df_test2[\"values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = []\n",
    "\n",
    "for row in ts:\n",
    "   tmp = []\n",
    "\n",
    "   if len(row) == 0:\n",
    "       continue\n",
    "\n",
    "   for item in row:\n",
    "       tmp.append(item)\n",
    "   fix.append(tmp)\n",
    "\n",
    "data = sc.parallelize(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "b = 25\n",
    "c = 2\n",
    "p = 65537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lsh.run(data, p, m, n, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1349 clusters.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s clusters.' % model.buckets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}